{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGRywNgES4l0"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('/content/VAL_Q3_Save your spot_Page_3.csv')\n",
        "\n",
        "target_col = 'Event count'\n",
        "\n",
        "print(data.isnull().sum())\n",
        "data[target_col].fillna(0, inplace=True)\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Data preprocessing\n",
        "# Since LSTM requires sequence data, we'll use a time window to create sequences.\n",
        "def create_sequences(data, window_size):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size):\n",
        "        X.append(data[i:i+window_size])\n",
        "        y.append(data[i+window_size])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Extract the 'Views' column as the target variable to forecast\n",
        "data = data[target_col].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data (optional, but recommended for better model performance)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_normalized = scaler.fit_transform(data)\n",
        "\n",
        "# Choose the time window size for creating sequences\n",
        "window_size = 30  # You can experiment with different window sizes\n",
        "\n",
        "# Create sequences for training\n",
        "X, y = create_sequences(data_normalized, window_size)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(0.8 * len(X))\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n"
      ],
      "metadata": {
        "id": "-turRQ7tTCEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb55262-6a2d-4d14-96d6-c81d3285d499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date           0\n",
            "Event count    1\n",
            "dtype: int64\n",
            "Date           0\n",
            "Event count    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Create the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, input_shape=(window_size, 1)))\n",
        "model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0q8THEeTblq",
        "outputId": "5e2fff92-e93a-44ee-fd8e-38873d30734e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 64)                16896     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16961 (66.25 KB)\n",
            "Trainable params: 16961 (66.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 100  # You can experiment with different numbers of epochs\n",
        "batch_size = 32  # You can experiment with different batch sizes\n",
        "\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mBGyw8ITg6o",
        "outputId": "a00511ec-2329-4d53-dc79-a7baf5df62e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 3s 32ms/step - loss: 0.0394\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.0207\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0179\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0181\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0174\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.0175\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0174\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 23ms/step - loss: 0.0174\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0173\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0172\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0172\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0171\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0171\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0172\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0171\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0170\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0170\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.0173\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.0168\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 23ms/step - loss: 0.0176\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.0175\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0172\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0171\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0169\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0168\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 34ms/step - loss: 0.0168\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 21ms/step - loss: 0.0169\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 23ms/step - loss: 0.0169\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0169\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.0167\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0168\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0168\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0168\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 29ms/step - loss: 0.0167\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0167\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0170\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0168\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0167\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0169\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0169\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0169\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.0168\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.0170\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 38ms/step - loss: 0.0171\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0167\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0168\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0168\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0168\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0167\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0166\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.0167\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0168\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0166\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0168\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 24ms/step - loss: 0.0175\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0173\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0169\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 26ms/step - loss: 0.0168\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0167\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0167\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 28ms/step - loss: 0.0166\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 30ms/step - loss: 0.0168\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0167\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0165\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.0168\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 58ms/step - loss: 0.0168\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 33ms/step - loss: 0.0166\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 23ms/step - loss: 0.0167\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0169\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 23ms/step - loss: 0.0167\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 25ms/step - loss: 0.0167\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0166\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0165\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 32ms/step - loss: 0.0168\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0166\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 35ms/step - loss: 0.0165\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 37ms/step - loss: 0.0167\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 36ms/step - loss: 0.0167\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0166\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 31ms/step - loss: 0.0164\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0165\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0166\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0163\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0165\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0165\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 14ms/step - loss: 0.0164\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0165\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.0162\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0166\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0163\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0162\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0161\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 16ms/step - loss: 0.0166\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0158\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0160\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0167\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0162\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 15ms/step - loss: 0.0164\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0159\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 13ms/step - loss: 0.0160\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x795e2334ead0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the testing data\n",
        "test_loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss}')\n",
        "\n",
        "# Add the code to compare test values and predicted values and calculate MAPE here\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the test data and predictions to their original scales\n",
        "y_test_original = scaler.inverse_transform(y_test)\n",
        "y_pred_original = scaler.inverse_transform(y_pred)\n",
        "\n",
        "# Create a DataFrame to compare test values and predicted values\n",
        "comparison_df = pd.DataFrame({'Actual': y_test_original.ravel(), 'Predicted': y_pred_original.ravel()})\n",
        "\n",
        "# Calculate the Mean Absolute Percentage Error (MAPE)\n",
        "def calculate_mape(actual, predicted):\n",
        "    return (1/len(actual)) * np.sum(np.abs((actual - predicted) / actual)) * 100\n",
        "\n",
        "mape = calculate_mape(y_test_original, y_pred_original)\n",
        "\n",
        "# Print the comparison DataFrame and MAPE\n",
        "print(comparison_df)\n",
        "print(f'MAPE: {mape:.2f}%')"
      ],
      "metadata": {
        "id": "2-ecBhmwaebr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have the date range from April 1, 2023, to June 30, 2023\n",
        "date_range = pd.date_range(start='2023-07-01', periods=3, freq='M')\n",
        "\n",
        "# Forecast the next three months (assuming the last window_size data points as the starting point)\n",
        "last_window = data_normalized[-window_size:].reshape(1, window_size, 1)\n",
        "forecasts_normalized = []\n",
        "\n",
        "# Forecast each month separately and store the results in a list\n",
        "for i in range(3):\n",
        "    forecast_normalized = model.predict(last_window)\n",
        "    forecasts_normalized.append(forecast_normalized[0, 0])\n",
        "    last_window = np.append(last_window[:, 1:, :], forecast_normalized.reshape(1, 1, 1), axis=1)\n",
        "\n",
        "# Inverse transform the forecasted values to get the actual scale\n",
        "forecasts = scaler.inverse_transform(np.array(forecasts_normalized).reshape(-1, 1))\n",
        "\n",
        "# Create a DataFrame to store the predictions\n",
        "predictions_df = pd.DataFrame({'Date': date_range, 'Predicted Event Count': forecasts.ravel()})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(predictions_df)\n"
      ],
      "metadata": {
        "id": "2PoQE2sO_CHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25bac4d7-2b02-4af8-87bd-b95bed3070ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "        Date  Predicted Event Count\n",
            "0 2023-07-31              36.058701\n",
            "1 2023-08-31              38.167961\n",
            "2 2023-09-30              39.078854\n"
          ]
        }
      ]
    }
  ]
}